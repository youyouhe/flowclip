from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from app.core.database import get_db
from app.core.security import get_current_user
from app.models.user import User
from app.models.video import Video
from app.models.project import Project
from app.services.minio_client import minio_service
from app.services.llm_service import llm_service
from app.core.config import settings
import os
import tempfile
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

router = APIRouter(
    tags=["llm"],
    responses={404: {"description": "æœªæ‰¾åˆ°"}},
)

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    video_id: Optional[int] = None
    system_prompt: Optional[str] = None
    use_srt_context: bool = False

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str
    usage: Optional[Dict[str, Any]] = None
    model: str
    video_context_used: bool = False

class SystemPromptRequest(BaseModel):
    """ç³»ç»Ÿæç¤ºè¯è¯·æ±‚æ¨¡å‹"""
    system_prompt: str

class SystemPromptResponse(BaseModel):
    """ç³»ç»Ÿæç¤ºè¯å“åº”æ¨¡å‹"""
    message: str
    current_prompt: str

@router.get("/test-long-request", operation_id="test_long_request")
async def test_long_request(request: Request):
    """
    æµ‹è¯•é•¿æ—¶é—´è¯·æ±‚ - ç”¨äºè¯Šæ–­ç½‘ç»œè¿æ¥é—®é¢˜
    æ¨¡æ‹ŸLLMè¯·æ±‚çš„å¤„ç†æ—¶é—´ï¼Œä½†ä¸å®é™…è°ƒç”¨LLMæœåŠ¡
    """
    import asyncio
    import time
    
    start_time = time.time()
    
    # è®°å½•è¯·æ±‚æ¥æºä¿¡æ¯
    client_ip = request.client.host
    user_agent = request.headers.get("user-agent", "Unknown")
    referer = request.headers.get("referer", "No referer")
    
    logger.info(f"ğŸš€ å¼€å§‹é•¿æ—¶é—´è¯·æ±‚æµ‹è¯• - {start_time}")
    logger.info(f"ğŸ” è¯·æ±‚æ¥æº: IP={client_ip}, UA={user_agent}, Referer={referer}")
    
    # æ¨¡æ‹ŸLLMå¤„ç†æ—¶é—´ï¼ˆ60ç§’ï¼‰
    logger.info("â³ å¼€å§‹60ç§’ç¡çœ ...")
    await asyncio.sleep(60)
    logger.info("âœ… 60ç§’ç¡çœ å®Œæˆ")
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    logger.info(f"ğŸ‰ æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {processing_time:.2f}ç§’")
    
    return {
        "success": True,
        "message": "é•¿æ—¶é—´è¯·æ±‚æµ‹è¯•å®Œæˆ",
        "processing_time_seconds": round(processing_time, 2),
        "start_time": start_time,
        "end_time": end_time
    }

@router.post("/chat", response_model=ChatResponse,
    summary="ä¸LLMå¯¹è¯",
    description="ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚æ”¯æŒä½¿ç”¨è§†é¢‘å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æï¼Œä¹Ÿå¯ä»¥è¿›è¡Œç®€å•çš„å¯¹è¯ã€‚",
    responses={
        200: {
            "description": "æˆåŠŸè¿”å›LLMå“åº”",
            "content": {
                "application/json": {
                    "example": {
                        "response": "è¿™æ˜¯LLMçš„å›å¤å†…å®¹",
                        "usage": {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30},
                        "model": "google/gemini-2.5-flash",
                        "video_context_used": True
                    }
                }
            }
        },
        404: {"description": "è§†é¢‘æœªæ‰¾åˆ°"},
        500: {"description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    }
, operation_id="llm_chat")
async def chat_with_llm(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    ä¸LLMå¯¹è¯
    
    ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚æ”¯æŒä½¿ç”¨è§†é¢‘å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æï¼Œä¹Ÿå¯ä»¥è¿›è¡Œç®€å•çš„å¯¹è¯ã€‚
    å¦‚æœè¯·æ±‚ä¸­åŒ…å«video_idä¸”use_srt_contextä¸ºTrueï¼Œåˆ™ä¼šä½¿ç”¨è§†é¢‘çš„SRTå­—å¹•å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
    
    Args:
        request (ChatRequest): èŠå¤©è¯·æ±‚å‚æ•°
            - message (str): ç”¨æˆ·çš„èŠå¤©æ¶ˆæ¯
            - video_id (Optional[int]): è§†é¢‘IDï¼Œç”¨äºè§†é¢‘å†…å®¹åˆ†æ
            - system_prompt (Optional[str]): ç³»ç»Ÿæç¤ºè¯
            - use_srt_context (bool): æ˜¯å¦ä½¿ç”¨SRTå­—å¹•å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        current_user (User): å½“å‰è®¤è¯ç”¨æˆ·
        db (AsyncSession): æ•°æ®åº“ä¼šè¯ä¾èµ–
        
    Returns:
        ChatResponse: LLMå“åº”ç»“æœ
            - response (str): LLMçš„å›å¤å†…å®¹
            - usage (Optional[Dict[str, Any]]): ä»¤ç‰Œä½¿ç”¨æƒ…å†µ
            - model (str): ä½¿ç”¨çš„æ¨¡å‹åç§°
            - video_context_used (bool): æ˜¯å¦ä½¿ç”¨äº†è§†é¢‘ä¸Šä¸‹æ–‡
            
    Raises:
        HTTPException: å½“è§†é¢‘æœªæ‰¾åˆ°æˆ–LLMå¯¹è¯å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    
    logger.info(f"æ”¶åˆ°LLMèŠå¤©è¯·æ±‚ - ç”¨æˆ·: {current_user.id}, æ¶ˆæ¯: {request.message[:50]}...")
    logger.info(f"è¯·æ±‚å‚æ•° - video_id: {request.video_id}, use_srt_context: {request.use_srt_context}")
    
    try:
        srt_content = None
        video_context_used = False
        
        # å¦‚æœéœ€è¦ä½¿ç”¨SRTä¸Šä¸‹æ–‡ä¸”æä¾›äº†video_id
        if request.use_srt_context and request.video_id:
            logger.info(f"å°è¯•è·å–SRTæ–‡ä»¶ - video_id: {request.video_id}")
            
            # éªŒè¯è§†é¢‘å±äºå½“å‰ç”¨æˆ·
            stmt = select(Video).join(Project).where(
                Video.id == request.video_id,
                Project.user_id == current_user.id
            )
            result = await db.execute(stmt)
            video = result.scalar_one_or_none()
            
            if not video:
                logger.warning(f"è§†é¢‘æœªæ‰¾åˆ° - video_id: {request.video_id}, user_id: {current_user.id}")
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Video not found"
                )
            
            logger.info(f"æ‰¾åˆ°è§†é¢‘: {video.title}")
            
            # æ„å»ºSRTæ–‡ä»¶å¯¹è±¡åç§°
            srt_object_name = f"users/{current_user.id}/projects/{video.project_id}/subtitles/{video.id}.srt"
            logger.info(f"æ£€æŸ¥SRTæ–‡ä»¶: {srt_object_name}")
            
            # æ£€æŸ¥SRTæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_exists = await minio_service.file_exists(srt_object_name)
            logger.info(f"SRTæ–‡ä»¶å­˜åœ¨: {file_exists}")
            
            if file_exists:
                # ä¸‹è½½SRTæ–‡ä»¶å†…å®¹
                try:
                    logger.info("å¼€å§‹ä¸‹è½½SRTæ–‡ä»¶...")
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.srt', delete=False, encoding='utf-8') as temp_file:
                        # è·å–æ–‡ä»¶å†…å®¹
                        response = minio_service.internal_client.get_object(
                            settings.minio_bucket_name,
                            srt_object_name
                        )
                        
                        # è¯»å–SRTå†…å®¹
                        srt_content = response.read().decode('utf-8')
                        temp_file.write(srt_content)
                        temp_file_path = temp_file.name
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(temp_file_path)
                    video_context_used = True
                    logger.info(f"SRTæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(srt_content)}")
                    
                except Exception as e:
                    logger.error(f"è¯»å–SRTæ–‡ä»¶å¤±è´¥: {e}")
                    srt_content = None
        else:
            logger.info("ä¸ä½¿ç”¨SRTä¸Šä¸‹æ–‡æˆ–æœªæä¾›video_id")
        
        # æ ¹æ®æ˜¯å¦æœ‰SRTå†…å®¹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if srt_content and video_context_used:
            logger.info("ä½¿ç”¨è§†é¢‘å†…å®¹åˆ†ææ¨¡å¼")
            # ä½¿ç”¨è§†é¢‘å†…å®¹åˆ†æ
            llm_response = await llm_service.analyze_video_content(
                srt_content=srt_content,
                user_question=request.message,
                system_prompt=request.system_prompt
            )
        else:
            logger.info("ä½¿ç”¨ç®€å•å¯¹è¯æ¨¡å¼")
            # ç®€å•å¯¹è¯
            llm_response = await llm_service.simple_chat(
                message=request.message,
                system_prompt=request.system_prompt
            )
        
        logger.info(f"LLMå“åº”: {llm_response}")
        
        # æå–å›å¤å†…å®¹
        response_text = ""
        usage_info = None

        if 'choices' in llm_response and len(llm_response['choices']) > 0:
            choice = llm_response['choices'][0]
            if 'message' in choice and 'content' in choice['message']:
                # ä¼˜å…ˆä½¿ç”¨è§£æåçš„JSONå†…å®¹ï¼Œæœ¬åœ°ä¿®æ”¹
                if 'parsed_content' in llm_response:
                    logger.info("ä½¿ç”¨è§£æåçš„JSONå†…å®¹")
                    if isinstance(llm_response['parsed_content'], str):
                        response_text = llm_response['parsed_content']
                    else:
                        # å¦‚æœæ˜¯JSONå¯¹è±¡ï¼Œæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                        import json
                        response_text = json.dumps(llm_response['parsed_content'], ensure_ascii=False, indent=2)
                else:
                    # å¦‚æœæ²¡æœ‰è§£æåçš„å†…å®¹ï¼Œä½¿ç”¨åŸå§‹content
                    response_text = choice['message']['content']
        
        if 'usage' in llm_response:
            usage_info = llm_response['usage']
        
        logger.info(f"èŠå¤©è¯·æ±‚æˆåŠŸ - å“åº”é•¿åº¦: {len(response_text)}")
        
        return ChatResponse(
            response=response_text,
            usage=usage_info,
            model=llm_response.get('model', 'unknown'),
            video_context_used=video_context_used
        )
        
    except Exception as e:
        logger.error(f"LLMèŠå¤©è¯·æ±‚å¤±è´¥: {type(e).__name__}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"LLMå¯¹è¯å¤±è´¥: {str(e)}"
        )

@router.post("/system-prompt", response_model=SystemPromptResponse,
    summary="æ›´æ–°ç³»ç»Ÿæç¤ºè¯",
    description="æ›´æ–°LLMå¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯ã€‚è¯¥è®¾ç½®ä»…åœ¨å½“å‰ä¼šè¯ä¸­ç”Ÿæ•ˆã€‚",
    responses={
        200: {
            "description": "æˆåŠŸæ›´æ–°ç³»ç»Ÿæç¤ºè¯",
            "content": {
                "application/json": {
                    "example": {
                        "message": "ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°ï¼Œå°†åœ¨ä¸‹æ¬¡å¯¹è¯ä¸­ç”Ÿæ•ˆ",
                        "current_prompt": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."
                    }
                }
            }
        },
        500: {"description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    }
, operation_id="update_system_prompt")
async def update_system_prompt(
    request: SystemPromptRequest,
    current_user: User = Depends(get_current_user)
):
    """
    æ›´æ–°ç³»ç»Ÿæç¤ºè¯ï¼ˆä»…ç”¨äºå½“å‰ä¼šè¯ï¼‰
    
    æ›´æ–°LLMå¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯ã€‚è¯¥è®¾ç½®ä»…åœ¨å½“å‰ä¼šè¯ä¸­ç”Ÿæ•ˆã€‚
    
    Args:
        request (SystemPromptRequest): ç³»ç»Ÿæç¤ºè¯è¯·æ±‚å‚æ•°
            - system_prompt (str): æ–°çš„ç³»ç»Ÿæç¤ºè¯
        current_user (User): å½“å‰è®¤è¯ç”¨æˆ·
        
    Returns:
        SystemPromptResponse: æ›´æ–°ç»“æœ
            - message (str): æ“ä½œç»“æœæ¶ˆæ¯
            - current_prompt (str): å½“å‰è®¾ç½®çš„æç¤ºè¯ï¼ˆæˆªå–å‰100ä¸ªå­—ç¬¦ï¼‰
            
    Raises:
        HTTPException: å½“æ›´æ–°å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    
    try:
        # è¿™é‡Œåªæ˜¯è¿”å›æˆåŠŸæ¶ˆæ¯ï¼Œå®é™…çš„ç³»ç»Ÿæç¤ºè¯åœ¨æ¯ä¸ªè¯·æ±‚ä¸­ä¼ é€’
        # å› ä¸ºç¯å¢ƒå˜é‡ä¸èƒ½åŠ¨æ€æ›´æ–°
        return SystemPromptResponse(
            message="ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°ï¼Œå°†åœ¨ä¸‹æ¬¡å¯¹è¯ä¸­ç”Ÿæ•ˆ",
            current_prompt=request.system_prompt[:100] + "..." if len(request.system_prompt) > 100 else request.system_prompt
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ›´æ–°ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {str(e)}"
        )

@router.get("/system-prompt",
    summary="è·å–å½“å‰ç³»ç»Ÿæç¤ºè¯",
    description="è·å–å½“å‰è®¾ç½®çš„LLMç³»ç»Ÿæç¤ºè¯ã€‚",
    responses={
        200: {
            "description": "æˆåŠŸè¿”å›å½“å‰ç³»ç»Ÿæç¤ºè¯",
            "content": {
                "application/json": {
                    "example": {
                        "system_prompt": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."
                    }
                }
            }
        },
        500: {"description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    }
, operation_id="get_system_prompt")
async def get_current_system_prompt():
    """
    è·å–å½“å‰ç³»ç»Ÿæç¤ºè¯
    
    è·å–å½“å‰è®¾ç½®çš„LLMç³»ç»Ÿæç¤ºè¯ã€‚
    
    Returns:
        dict: åŒ…å«å½“å‰ç³»ç»Ÿæç¤ºè¯çš„å­—å…¸
            - system_prompt (str): å½“å‰çš„ç³»ç»Ÿæç¤ºè¯
            
    Raises:
        HTTPException: å½“è·å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    
    try:
        from app.services.llm_service import llm_service
        # è·å–å½“å‰é…ç½®è€Œä¸æ˜¯ç›´æ¥è®¿é—®ä¸å­˜åœ¨çš„å±æ€§
        config = llm_service._get_current_config()
        return {
            "system_prompt": config['default_system_prompt']
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {str(e)}"
        )

@router.get("/models",
    summary="è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨",
    description="è·å–LLMæœåŠ¡ä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚è¯¥æ¥å£ä¼šä»OpenRouter APIåŠ¨æ€è·å–Googleæ¨¡å‹åˆ—è¡¨ï¼Œå¦‚æœè·å–å¤±è´¥åˆ™è¿”å›é»˜è®¤æ¨¡å‹ã€‚",
    responses={
        200: {
            "description": "æˆåŠŸè¿”å›æ¨¡å‹åˆ—è¡¨",
            "content": {
                "application/json": {
                    "example": {
                        "models": [
                            {
                                "id": "google/gemini-2.5-flash",
                                "name": "Gemini 2.5 Flash",
                                "description": "é»˜è®¤ä½¿ç”¨çš„Geminiæ¨¡å‹"
                            }
                        ]
                    }
                }
            }
        },
        500: {"description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    }
, operation_id="get_models")
async def get_available_models():
    """
    è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    
    è·å–LLMæœåŠ¡ä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚è¯¥æ¥å£ä¼šä»OpenRouter APIåŠ¨æ€è·å–Googleæ¨¡å‹åˆ—è¡¨ï¼Œ
    å¦‚æœè·å–å¤±è´¥åˆ™è¿”å›é»˜è®¤æ¨¡å‹ã€‚
    
    Returns:
        dict: åŒ…å«æ¨¡å‹åˆ—è¡¨çš„å­—å…¸
            - models (List[Dict]): æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
                - id (str): æ¨¡å‹ID
                - name (str): æ¨¡å‹åç§°
                - description (str): æ¨¡å‹æè¿°
                
    Raises:
        HTTPException: å½“è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    
    try:
        # ä»OpenRouter APIåŠ¨æ€è·å–Googleæ¨¡å‹åˆ—è¡¨
        openrouter_models = await llm_service.get_available_models(filter_provider="google")
        
        # å¦‚æœæ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
        if not openrouter_models:
            logger.warning("æ— æ³•ä»OpenRouterè·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›é»˜è®¤æ¨¡å‹")
            return {
                "models": [
                    {
                        "id": "google/gemini-2.5-flash",
                        "name": "Gemini 2.5 Flash",
                        "description": "é»˜è®¤ä½¿ç”¨çš„Geminiæ¨¡å‹"
                    }
                ]
            }
        
        # è½¬æ¢æ¨¡å‹æ ¼å¼
        models = []
        for model in openrouter_models:
            models.append({
                "id": model.get("id"),
                "name": model.get("name", model.get("id")),
                "description": model.get("description", "")
            })
        
        return {
            "models": models
        }
        
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {type(e).__name__}: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
        return {
            "models": [
                {
                    "id": "google/gemini-2.5-flash",
                    "name": "Gemini 2.5 Flash",
                    "description": "é»˜è®¤ä½¿ç”¨çš„Geminiæ¨¡å‹"
                }
            ]
        }